#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆPubMedæµ‹è¯•è„šæœ¬
å…ˆæµ‹è¯•åŸºæœ¬åŠŸèƒ½æ˜¯å¦æ­£å¸¸
"""

import requests
from bs4 import BeautifulSoup
import json

def test_xml_parser():
    """æµ‹è¯•XMLè§£æå™¨æ˜¯å¦å¯ç”¨"""
    try:
        # æµ‹è¯•lxml
        from bs4 import BeautifulSoup
        test_xml = "<root><test>Hello</test></root>"
        soup = BeautifulSoup(test_xml, 'xml')
        print("âœ“ XMLè§£æå™¨æµ‹è¯•æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âœ— XMLè§£æå™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_pubmed_api():
    """æµ‹è¯•PubMed APIè¿æ¥"""
    print("æµ‹è¯•PubMed APIè¿æ¥...")
    
    try:
        # ç®€å•çš„APIæµ‹è¯•
        url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
        params = {
            'db': 'pubmed',
            'term': 'cancer',
            'retmax': 5,
            'retmode': 'json'
        }
        
        response = requests.get(url, params=params, timeout=10)
        response.raise_for_status()
        
        data = response.json()
        pmids = data.get('esearchresult', {}).get('idlist', [])
        
        if pmids:
            print(f"âœ“ APIè¿æ¥æˆåŠŸï¼Œæ‰¾åˆ°PMIDs: {pmids[:3]}...")
            return pmids[:3]  # è¿”å›å‰3ä¸ªPMIDç”¨äºè¿›ä¸€æ­¥æµ‹è¯•
        else:
            print("âœ— APIè¿æ¥æˆåŠŸä½†æ²¡æœ‰æ‰¾åˆ°ç»“æœ")
            return []
            
    except Exception as e:
        print(f"âœ— APIè¿æ¥å¤±è´¥: {e}")
        return []

def test_fetch_articles(pmids):
    """æµ‹è¯•è·å–å…·ä½“æ–‡ç« ä¿¡æ¯"""
    if not pmids:
        print("æ²¡æœ‰PMIDå¯ä»¥æµ‹è¯•")
        return
    
    print("æµ‹è¯•è·å–æ–‡ç« è¯¦æƒ…...")
    
    try:
        url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
        params = {
            'db': 'pubmed',
            'id': ','.join(pmids),
            'retmode': 'xml'
        }
        
        response = requests.get(url, params=params, timeout=15)
        response.raise_for_status()
        
        # å°è¯•è§£æXML
        soup = BeautifulSoup(response.content, 'xml')
        articles = soup.find_all('PubmedArticle')
        
        print(f"âœ“ æˆåŠŸè·å– {len(articles)} ç¯‡æ–‡ç« ")
        
        # æ˜¾ç¤ºç¬¬ä¸€ç¯‡æ–‡ç« çš„ä¿¡æ¯
        if articles:
            first_article = articles[0]
            
            # è·å–æ ‡é¢˜
            title_elem = first_article.find('ArticleTitle')
            title = title_elem.text if title_elem else "æ— æ ‡é¢˜"
            
            # è·å–PMID
            pmid_elem = first_article.find('PMID')
            pmid = pmid_elem.text if pmid_elem else "æ— PMID"
            
            print(f"ç¬¬ä¸€ç¯‡æ–‡ç« :")
            print(f"PMID: {pmid}")
            print(f"æ ‡é¢˜: {title[:100]}...")
            
            return True
        else:
            print("âœ— æ²¡æœ‰è§£æåˆ°æ–‡ç« å†…å®¹")
            return False
            
    except Exception as e:
        print(f"âœ— è·å–æ–‡ç« è¯¦æƒ…å¤±è´¥: {e}")
        return False

def test_web_scraping():
    """æµ‹è¯•ç½‘é¡µçˆ¬å–æ–¹æ³•"""
    print("æµ‹è¯•ç½‘é¡µçˆ¬å–æ–¹æ³•...")
    
    try:
        url = "https://pubmed.ncbi.nlm.nih.gov/?term=cancer&size=5"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # å°è¯•å¤šç§å¯èƒ½çš„æ ‡é¢˜é€‰æ‹©å™¨
        title_selectors = [
            'a.docsum-title',
            'a[data-article-id]',
            'h1.heading-title',
            'a[href*="/"]'
        ]
        
        titles_found = []
        for selector in title_selectors:
            elements = soup.select(selector)
            if elements:
                for elem in elements[:3]:
                    text = elem.get_text().strip()
                    if len(text) > 20:  # è¿‡æ»¤æ‰å¤ªçŸ­çš„æ–‡æœ¬
                        titles_found.append(text)
                        
        if titles_found:
            print(f"âœ“ ç½‘é¡µçˆ¬å–æˆåŠŸï¼Œæ‰¾åˆ° {len(titles_found)} ä¸ªæ ‡é¢˜")
            for i, title in enumerate(titles_found[:3], 1):
                print(f"{i}. {title[:80]}...")
            return True
        else:
            print("âœ— ç½‘é¡µçˆ¬å–å¤±è´¥ï¼Œæ²¡æœ‰æ‰¾åˆ°æ ‡é¢˜")
            # æ˜¾ç¤ºé¡µé¢å†…å®¹ç‰‡æ®µç”¨äºè°ƒè¯•
            page_text = soup.get_text()[:300]
            print(f"é¡µé¢å†…å®¹é¢„è§ˆ: {page_text}")
            return False
            
    except Exception as e:
        print(f"âœ— ç½‘é¡µçˆ¬å–å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("PubMedåŠŸèƒ½æµ‹è¯•")
    print("=" * 40)
    
    # 1. æµ‹è¯•XMLè§£æå™¨
    xml_ok = test_xml_parser()
    print()
    
    # 2. æµ‹è¯•APIè¿æ¥
    pmids = test_pubmed_api()
    print()
    
    # 3. å¦‚æœæœ‰PMIDï¼Œæµ‹è¯•è·å–è¯¦æƒ…
    if xml_ok and pmids:
        fetch_ok = test_fetch_articles(pmids)
        print()
    else:
        fetch_ok = False
    
    # 4. æµ‹è¯•ç½‘é¡µçˆ¬å–
    web_ok = test_web_scraping()
    print()
    
    # æ€»ç»“
    print("æµ‹è¯•ç»“æœæ€»ç»“:")
    print(f"XMLè§£æå™¨: {'âœ“' if xml_ok else 'âœ—'}")
    print(f"APIè¿æ¥: {'âœ“' if pmids else 'âœ—'}")
    print(f"è·å–è¯¦æƒ…: {'âœ“' if fetch_ok else 'âœ—'}")
    print(f"ç½‘é¡µçˆ¬å–: {'âœ“' if web_ok else 'âœ—'}")
    
    if xml_ok and (pmids or web_ok):
        print("\nğŸ‰ åŸºæœ¬åŠŸèƒ½æ­£å¸¸ï¼Œå¯ä»¥è¿è¡Œå®Œæ•´ç¨‹åºï¼")
    else:
        print("\nğŸ”§ éœ€è¦è§£å†³ä»¥ä¸‹é—®é¢˜:")
        if not xml_ok:
            print("- å®‰è£…XMLè§£æå™¨: pip install lxml")
        if not pmids and not web_ok:
            print("- æ£€æŸ¥ç½‘ç»œè¿æ¥")
            print("- å¯èƒ½éœ€è¦VPNè®¿é—®PubMed")

if __name__ == "__main__":
    main()